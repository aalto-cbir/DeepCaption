## Supported features - Training

### Teacher forcing scheduling

Teacher forcing in RNNs refers to feeding ground truth token at time step `t`. [This paper](https://arxiv.org/abs/1506.03099) details an approach where at each time step `t` ground truth is fed with probability `P(teacher_forcing)` and the token generated by the network itself with probability `1 - P(teacher_forcing)`. Currently inverse sigmoid sampling schedule is implemented for the above probabilities.

To train a model with sampled teacher forcing using `k=2200` and `beta=0.3` run the following:

```bash
$ python train.py --dataset coco:train2014 --model_name my_model --teacher_forcing sampled --teacher_forcing_k 2200 --teaacher_forcing_beta 0.3
```

This inverse sigmoid scheduling implementation depends on the parameter `k` which is usually in the order of `1000s` and can be interpreted as "how soon do we want to start decreasing the probability of teacher forcing?" and parameter `beta` which between `0` and `1` and can be interpreted as "once we start to use model's own outputs, how fast do we want the rate of model outputs usage to increase?", intuitively this is the slope of the middle segment of the inverse sigmoid curve.

Teacher forcing is controlled by parameter `--teacher_forcing`. By default this is set to `always`, meaning that we don't perform any sampling. Other options are `sampled` - using a sampling procedure outlined above; and `additive` - deterministic summation of teacher token with generated token with weights determined by the inverse sigmoid scheduler.

`--teacher_forcing_k` sets the value of `k` and `--teacher_forcing_beta` sets the value for `beta`.

# Feature Extraction

You can use `extract_dataset_features.py` to extract features from one of the convolutional models made available in `models.py`. Currently the following CNN models from PyTorch `torchvision` are supported `alexnet`, `Densenet 20`, `Resnet-152`, `VGG-16`, and `Inception V3`, all trained on ImageNet classification task. The exctracted features are either taken from the already flattened pre-classification layer, or by flattening the final convolutional or pooling layer.

The resulting features are saved using `lmdb` file format. Example command for generating features computed from images in MS-COCO training and validation sets using ResNet-152 CNN:

```bash
$ python extract_dataset_features.py --dataset coco:train2014+coco:val2014 --extractor resnet152
```

Feature extraction script currently supports the feature types specified by `--feature_type`:

* **plain** - takes an input image, resizes it and calculates features without any augmentation
* **avg** - takes 5 different crops of a resized input image - 4 corners + center, and then flips each crop horizontally, producing in total 10 cropped images. These images are then processed by the specified CNN separately, and the resulting single feature vector output produced by the feature extractor is formed by applying elementwise avareging over 10 feature vectors
* **max** - same as **avg**, but using elementwise maximum

Three different pixel value normalization strategies are currently supported for `avg` and `max` feature types. Normalization is specified by `--normalize` parameter:

* **default** - applies per-channel normalization settings [recommended](https://pytorch.org/docs/stable/torchvision/models.html) by PyTorch
* **skip** - do not normalize pixel values
* **substract_half** - subtract `0.5` from each pixel value, after the pixel values have been converted to be between `0` and `1`.

Feature extractor supports the same dataset configuration format as the `train.py` and `infer.py` scripts.
