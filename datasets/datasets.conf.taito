[coco]
dataset_class = CocoDataset
root_dir = /proj/mediaind/picsom/databases/COCO
vocab_path = image_captioning/vocab.pkl
features_path = features/

[coco:train2014]
#image_dir = image_captioning/train2014_256x256
image_dir = download/images/train2014
caption_path = download/annotations/captions_train2014.json

[coco:val2014]
#image_dir = image_captioning/val2014_256x256
image_dir = download/images/val2014
caption_path = download/annotations/captions_val2014.json

[coco:train2017]
caption_path = download/annotations/captions_train2017.json

[coco:val2017]
caption_path = download/annotations/captions_val2017.json
image_dir = image_captioning/val2014_256x256

[vgim2p]
# Visual Genome paragraph captions
dataset_class = VisualGenomeIM2PDataset
root_dir = /proj/mediaind/picsom/databases/visualgenome
image_dir = image_captioning/images_256x256
caption_path = download/im2p/paragraphs_v1.json
# We use COCO vocab for pre-training compatibility:
vocab_path = /proj/mediaind/picsom/databases/COCO/image_captioning/vocab.pkl
features_path = features

[vgim2p:train]
subset = /proj/mediaind/picsom/databases/visualgenome/download/im2p/train_split.json

[vgim2p:val]
subset = /proj/mediaind/picsom/databases/visualgenome/download/im2p/val_split.json

[vgim2p:test]
subset = /proj/mediaind/picsom/databases/visualgenome/download/im2p/test_split.json

[msrvtt]
dataset_class = MSRVTTDataset
root_dir = /proj/mediaind/picsom/databases/MSR-VTT
image_dir = middleframes/resized/
caption_path = download/train_val_videodatainfo.json
vocab_path = image_captioning/vocab.pkl
features_path = features/

[msrvtt:train]
subset = train

[msrvtt:validate]
# MSR-VTT validation set
subset = validate

[trecvid2018]
dataset_class = TRECVID2018Dataset
root_dir = /proj/mediaind/picsom/databases/trecvid2018
image_dir = middleframes/
features_path = features/

[trecvid2016]
